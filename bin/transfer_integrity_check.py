#!/usr/bin/env python3

import subprocess
import pandas as pd
import argparse
from argparse import RawTextHelpFormatter
import os
import fnmatch
import glob
import hashlib
import gzip
import sys
from datetime import datetime

description = """
This script transfers all files from a user's COVID upload area to the drag and drop directory on codon (or a user specified directory)
and compares checksums to verify file integrity. 
"""

example = """
Example 1:
    python3 transfer_integrity_check.py -u 65352cc8-c581-43e0-8451-8eddbfd39a5d 

Example 2:
    python3 transfer_integrity_check.py -u 65352cc8-c581-43e0-8451-8eddbfd39a5d -o <my output dir>
    
Example 3:
    python3 transfer_integrity_check.py -u 65352cc8-c581-43e0-8451-8eddbfd39a5d -s

Example 4:
    python3 transfer_integrity_check.py -u 65352cc8-c581-43e0-8451-8eddbfd39a5d -d
"""

# -------- #
# Parsing arguments
# -------- #

parser = argparse.ArgumentParser(description=description, epilog=example, formatter_class=RawTextHelpFormatter)

parser.add_argument('-u', '--UUID', help='User specific access key for drag and drop tool', type=str, required=True)

# adding optional output argument for future use/flexibility. Default output directory = /nfs/production/cochrane/ena/users/covid/covid/{args.UUID}'
parser.add_argument('-o', '--output', help='Specify output directory to transfer files from S3 bucket to, and hold files + subdirs generated by the script', type=str, required=False)  # False #No trailing slash

# the following 2 arguments are mutually exclusive, but neither are required. If none are supplied, both data files and user spreadsheets will be transferred to codon
group = parser.add_argument_group('Data/metadata specific flags', 'additional arguments to transfer either data files OR user metadata spreadsheets')
exclusive_group = group.add_mutually_exclusive_group()

exclusive_group.add_argument('-s', '--spreadsheet_only', help='optional flag to transfer only the latest user metadata spreadsheet to the cluster', action='store_true', required=False) #cannot use type='str' parameter with 'store_true'
exclusive_group.add_argument('-d', '--data_files_only', help='optional flag to transfer only data files (no user metadata spreadsheets) to the cluster', action='store_true', required=False) #cannot use type='str' parameter with 'store_true'

args = parser.parse_args()
#parser.print_help()

# -------- #
# Hard-coded variables
# -------- #

# Data file extensions accepted by tool
file_exts = ('*fasta*', '*fastq*', '*cram*', '*bam*')
sheet_exts = ('*.xls*', '*.tsv*', '*.csv*', '*.txt*')
subdirs = ['pass', 'fail', 'input_spreadsheet', 'md5']
# Name of the columns in the summary dataframe
file_name = "file_name"
uploaded_md5_val = "uploaded_md5_value"
new_md5_val = "new_md5_value"
integrity_check = "file_integrity_check"
# Datetime string for summary tsv file
now = datetime.now()
dt_string = now.strftime("%y-%m-%d_%H-%M-%S")
# file type str to be transferred #TODO: check if this is needed
# s = "spreadsheets"
# d = "data" # data files


def setup_dirs():

    if args.output:
        outdir = f"{args.output}"  # user specified outdir
    else:
        outdir = f"/nfs/production/cochrane/ena/users/covid/covid/{args.UUID}"

    trans_int_outdir = "transfer_integrity_output" #subdir for output files

    if not os.path.exists(f"{outdir}/{trans_int_outdir}"):
        try:
            os.makedirs(f"{outdir}/{trans_int_outdir}")
            print(f"Successfully created output directory {outdir}/{trans_int_outdir}")
        except Exception as error:
            print(error)
            print(f"\n ERROR in transfer_integrity_check.py: could not create output directory '{outdir}/{trans_int_outdir}'. Exiting.", file=sys.stderr)
            sys.exit()

    return outdir, trans_int_outdir


def run_bash_cmds(cmds):
    if cmds == main.upload_cmds:
        with open(f'{main.outdir}/{main.trans_int_outdir}/upload_area_stdout.txt', 'w+') as stdout:
            subprocess.run(main.upload_cmds, shell=True, stdin=subprocess.PIPE, stdout=stdout, stderr=subprocess.PIPE, encoding='utf8')  # universal_newlines to accept input/output as str not bytes
            stdout.seek(0)
            print(stdout.read())

        return stdout.name # name of file

    elif cmds == main.transfer_cmds:
        with open(f'{main.outdir}/{main.trans_int_outdir}/file_transfer_log.txt', 'w+') as logfile:
            subprocess.run(main.transfer_cmds, shell=True, stdin=subprocess.PIPE, stdout=logfile, stderr=logfile, encoding='utf8')
            logfile.seek(0)
            print(logfile.read())

        return logfile.name

    elif cmds == main.md5sum:
        with open(f'{main.outdir}/{main.fname}.md5', 'w+') as fmd5:
            subprocess.run(main.md5sum, stdout=fmd5, shell=True, stderr=subprocess.PIPE, stdin=subprocess.PIPE, encoding='utf8')  # adding '' to account for file name
            fmd5.seek(0)  # to move cursor back to start of file for reading
            md5_contents = fmd5.read()
            print(md5_contents)  # to view contents of each .md5 file

        return md5_contents

    else:
        raise ValueError("ERROR with input bash commands, exiting", file=sys.stderr)
        sys.exit()


def md5_handling(md5_contents, filename):
    # isolate original and newly calculated hashes
    new_md5 = md5_contents.split()[0]  # extracts first col from .md5 file
    original_md5 = (os.path.splitext(f"{filename}")[1])[1:]  # extracts '.<hash>' from uploaded filename, and removes the leading '.'
    print(f"new md5 = ", new_md5), print(f"original md5 = ", original_md5, end='\n\n')  # print blank line at the end of print statement

    # rename submitted files to remove the hash (for downstream Webin-CLI submission)
    new_filename = os.path.splitext(f"{filename}")[0]
    # print(new_fname)
    # os.rename(f"{outdir}/{fname}", new_fname)
    os.rename(f"{main.outdir}/{filename}",f"{main.outdir}/{new_filename}")  # to overcome OSError: [Errno 18] Invalid cross-device link error

    return new_md5, original_md5, new_filename


def transfer_check(file):
    with open(f'{file}', 'r') as stdout:
        #total_upload = (stdout.readlines()[-1])  # prints last line of stdout = no. of items in upload area #14 items
        #print("total files uploaded =", total_upload)
        #
        # total_transfer = len([name for name in os.listdir(main.outdir) if os.path.isfile(os.path.join(main.outdir, name))])  # list no. of files in specified dir
        # total_transfer = str(total_transfer) + " items"

        if args.spreadsheet_only:
            print("----transferring user spreadsheets only----\n")
            #total_transfer = len([name for name in os.listdir(main.outdir) if any(fnmatch.fnmatch(name, sh_ext) for sh_ext in sheet_exts)])  # list no. of spreadsheet files transferred to dir
        elif args.data_files_only:
            print("----transferring data files only----\n")
            #total_transfer = len([name for name in os.listdir(main.outdir) if any(fnmatch.fnmatch(name, ext) for ext in file_exts)]) # list no. of spreadsheet data files transferred to dir

        #total_transfer = str(total_transfer) + " items"
        #print(f"files transferred to {main.outdir} = {total_transfer}\n") # TODO: remember if the script is re-run, this statement will not be accurate as passed files will be deleted


def setup_subdirs(subdir): # for pass, fail, input_spreadsheet, md5 subdirs
    if not os.path.exists(f'{main.outdir}/{main.trans_int_outdir}/{subdir}'):
        try:
            os.mkdir(f'{main.outdir}/{main.trans_int_outdir}/{subdir}')

        except Exception as error:
            print(error)
            sys.exit()

    move_files(subdir)

def move_files(subdir):
    try:
        if subdir == 'md5':
            for file in [f for f in os.listdir(os.path.abspath(main.outdir)) if fnmatch.fnmatch(f, '*.md5*')]:
                os.rename(f'{main.outdir}/{file}', f'{main.outdir}/{main.trans_int_outdir}/md5/{file}')  # moving md5 files to md5 dir

        elif subdir == 'pass':
            for pfile in main.summary_df.query('file_integrity_check == "PASS"')['file_name'].to_list():
                os.rename(f'{main.outdir}/{pfile}', f'{main.outdir}/{main.trans_int_outdir}/pass/{pfile}')

        elif subdir == 'fail':
            for ffile in main.summary_df.query('file_integrity_check == "FAIL"')['file_name'].to_list():
                os.rename(f'{main.outdir}/{ffile}', f'{main.outdir}/{main.trans_int_outdir}/fail/{ffile}')

        elif subdir == 'input_spreadsheet':
            i_sheets = []
            for sh_ext in sheet_exts:
                i_sheets += glob.glob(f'{main.outdir}/{sh_ext}') #without list ocmprehension as it adds a double '[['
            #print(i_sheets)

            datestrings = sorted([sheet.split(".")[-2] for sheet in i_sheets])  # extracts file datestrings (e.g. 2023-01-25T12:45:08) into sorted list
            latest_ds = str(datestrings[-1])  # extracts most recent datestring
            latest_file = (str([sheet for sheet in i_sheets if latest_ds in sheet])[1:-1]).strip('\'')  # removing [] and '' from list element
            print("latest file is ", latest_file)

            os.rename(f'{latest_file}', f'{main.outdir}/{main.trans_int_outdir}/input_spreadsheet/{os.path.basename(latest_file)}')

    except Exception as error:
        print(f"ERROR in transferring {subdir} to {subdir} directory. Exiting", file=sys.stderr)
        print(error)
        sys.exit()


def main():

    # Variables
    main.outdir, main.trans_int_outdir = setup_dirs() # runs the function and returns the variable/s
    #print(main.trans_int_outdir)

    main.upload_cmds = f"covid-util select {args.UUID}; covid-util list"

    if args.spreadsheet_only: # if we only want to transfer the user metadata spreadsheets from S3 bucket
        main.aws_cmd = f" && aws --endpoint-url https://uk1s3.embassy.ebi.ac.uk s3 cp s3://covid-utils-ui-88560523/{args.UUID} . --recursive --exclude '*' --include '*.xls*' --include '*.tsv*' --include '*.csv*' --include '*.txt*'"
    elif args.data_files_only: # if we only want to transfer the data files from S3 bucket
        main.aws_cmd = f" && aws --endpoint-url https://uk1s3.embassy.ebi.ac.uk s3 cp s3://covid-utils-ui-88560523/{args.UUID} . --recursive --include '*' --exclude '*.xls*' --exclude '*.tsv*' --exclude '*.csv*' --exclude '*.txt*'"
    else: # to transfer full contents of S3 bucket
        main.aws_cmd = f" && aws --endpoint-url https://uk1s3.embassy.ebi.ac.uk s3 cp s3://covid-utils-ui-88560523/{args.UUID} . --recursive" # to transfer all files

    if args.output:
        main.transfer_cmds = f"cd {main.outdir}" + main.aws_cmd
        print(main.transfer_cmds)
    else:
        main.transfer_cmds = f"cd {os.path.dirname(main.outdir)}; cd venv/; source bin/activate && cd {main.outdir}" + main.aws_cmd # default dir on codon
        print(main.transfer_cmds)

    # Select and list files associated to specified UUID
    #run_bash_cmds(main.upload_cmds)
    stdout = run_bash_cmds(main.upload_cmds)

    # Writing to transfer log file
    run_bash_cmds(main.transfer_cmds)

    # Check all files have transferred correctly
    transfer_check(stdout)

    # # # -------- #
    # # # File integrity check
    # # # -------- #

    if not args.spreadsheet_only: # below code only to run if data files, or data files + spreadsheet will be transferred #TODO: ideally place below code in a function
        # Creating summary df to record integrity checks
        main.summary_df = pd.DataFrame(columns=[file_name, uploaded_md5_val, new_md5_val, integrity_check])

        # Obtaining a list of data files to submit
        submitted_files = []

        for main.fname in os.listdir(main.outdir):
            if any(fnmatch.fnmatch(main.fname, ext) for ext in file_exts):
                print(main.fname)
                submitted_files.append(main.fname)

                main.md5sum = f"md5sum '{main.outdir}/{main.fname}'"  # TODO - place elsewhere?

                # calculating md5 for each data file, and storing each in corresponding .md5 file
                md5_contents = run_bash_cmds(main.md5sum)

                # extracting hashes and renaming files to remove hash extension
                new_md5, original_md5, new_filename = md5_handling(md5_contents, main.fname)

                # We add the output of the pushed changes to the summary dataframe
                new_df = pd.DataFrame([{file_name: new_filename, uploaded_md5_val: original_md5, new_md5_val: new_md5,
                                        integrity_check: "PASS" if new_md5 == original_md5 else "FAIL"}])
                main.summary_df = pd.concat([main.summary_df, new_df], axis=0, ignore_index=True)  # using concat as df.append is deprecated

        print(main.summary_df)

        # Create subdirs and move files to appropriate subdir
        if args.data_files_only:
            for dir in subdirs:
                if dir == 'input_spreadsheet':
                    continue  # do not create input_spreadsheet subdir
                setup_subdirs(dir)

        else: # i.e transfer all files from s3 bucket
            for dir in subdirs:
                setup_subdirs(dir)

        # error check that count of files in pass + fail dir = total in summary_df
        try:
            len(os.listdir(f'{main.outdir}/{main.trans_int_outdir}/pass')) + len(
                os.listdir(f'{main.outdir}/{main.trans_int_outdir}/fail')) == len(main.summary_df)
        except:
            print("ERROR in transferring files to PASS/FAIL output directories", file=sys.stderr)

        # creating output tsv file
        summary_file_basename = f"file_integrity_summary_{dt_string}.tsv"
        summary_filename = os.path.join(f'{main.outdir}/{main.trans_int_outdir}', summary_file_basename)
        main.summary_df.to_csv(path_or_buf=summary_filename, sep="\t", index=False)

        # check for any corrupt data files
        if (main.summary_df[integrity_check] == "PASS").all():
            print(f"\nAll {len(main.summary_df)} data files in upload area: {main.outdir} have passed verification")
        else:
            print(
                f"\nERROR: some data files in upload area: {main.outdir} failed verification. Please see summary file {summary_file_basename}")

    else: # i.e if we want only the user spreadsheet
        # no data file integrity check as data files have not been transferred
        setup_subdirs('input_spreadsheet')


main()
